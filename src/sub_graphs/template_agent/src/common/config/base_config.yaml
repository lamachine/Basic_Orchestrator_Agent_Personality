llm:
  ollama:
    api_url: "http://localhost:11434"
    default_model: "llama3.1:latest"
    timeout: 30
    settings:
      max_tokens: 2048
      context_window: 16384
      temperature: 0.7
      top_p: 0.9
      frequency_penalty: 0.0
      presence_penalty: 0.0
    models:
      embedding:
        model: "nomic-embed-text"
        settings:
          max_tokens: 2048
          context_window: 16384
          temperature: 0.7
          top_p: 0.9
          frequency_penalty: 0.0
          presence_penalty: 0.0
        dimensions: 1536
        normalize: true
      programming:
        model: "llama3.1:latest"
        settings:
          max_tokens: 2048
          context_window: 16384
          temperature: 0.7
          top_p: 0.9
          frequency_penalty: 0.0
          presence_penalty: 0.0
      reasoning:
        model: "deepseek-r1"
        settings:
          max_tokens: 2048
          context_window: 16384
          temperature: 0.7
          top_p: 0.9
          frequency_penalty: 0.0
          presence_penalty: 0.0
      chat:
        model: "llama3.1:latest"
        settings:
          max_tokens: 2048
          context_window: 16384
          temperature: 0.7
          top_p: 0.9
          frequency_penalty: 0.0
          presence_penalty: 0.0
  openai:
    api_url: "https://api.openai.com/v1"
    default_model: "gpt-4"
    timeout: 30
    settings:
      max_tokens: 4096
      context_window: 8192
      temperature: 0.7
      top_p: 0.9
      frequency_penalty: 0.0
      presence_penalty: 0.0
    models:
      embedding:
        model: "text-embedding-ada-002"
        settings:
          max_tokens: 4096
          context_window: 8192
          temperature: 0.7
          top_p: 0.9
          frequency_penalty: 0.0
          presence_penalty: 0.0
        dimensions: 1536
        normalize: true
      programming:
        model: "gpt-4"
        settings:
          max_tokens: 4096
          context_window: 8192
          temperature: 0.7
          top_p: 0.9
          frequency_penalty: 0.0
          presence_penalty: 0.0
      reasoning:
        model: "gpt-4"
        settings:
          max_tokens: 4096
          context_window: 8192
          temperature: 0.7
          top_p: 0.9
          frequency_penalty: 0.0
          presence_penalty: 0.0
      chat:
        model: "gpt-4"
        settings:
          max_tokens: 4096
          context_window: 8192
          temperature: 0.7
          top_p: 0.9
          frequency_penalty: 0.0
          presence_penalty: 0.0

state:
  state_dir: "./state"
  state_file: "state.json"
  backup_count: 5
  auto_save: true
  save_interval: 300
  max_state_history: 100
  cleanup_interval: 3600
  compression_enabled: true
  encryption_enabled: false

session:
  session_timeout: 3600
  max_sessions: 100
  cleanup_interval: 300
  session_dir: "./sessions"
  session_file: "session.json"
  backup_count: 5
  auto_cleanup: true
  compression_enabled: true
  encryption_enabled: false
  session_types:
    - "user"
    - "system"
    - "temporary"
  retention_policy:
    user_sessions: "30 days"
    system_sessions: "7 days"
    temporary_sessions: "1 day"

logging:
  enable_logging: true
  log_level: "INFO"
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  log_file: "agent.log"
  log_to_console: true
  log_to_file: true
  log_rotation: "1 day"
  log_retention: "7 days"
  file_level: "INFO"
  console_level: "INFO"
  log_dir: "logs"
  max_log_size_mb: 10
  backup_count: 5
  formatters:
    default:
      format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    detailed:
      format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s - %(pathname)s:%(lineno)d"
  noisy_loggers: []

database:
  provider: "supabase_local"
  providers:
    supabase_local:
      url: "http://localhost:54321"
      anon_key: "your-anon-key"
      service_role_key: "your-service-role-key"
    supabase_web:
      url: "https://your-project.supabase.co"
      anon_key: "your-anon-key"
      service_role_key: "your-service-role-key"
    postgres:
      url: "postgresql://postgres:postgres@localhost:5432/postgres"
  pool_size: 5
  max_overflow: 10
  echo: false
  pool_timeout: 30
  pool_recycle: 3600

graph:
  name: "template_agent"
  description: "Template agent for basic orchestration"
  version: "1.0.0"
  max_depth: 10
  max_breadth: 5
  timeout: 300
  retry_count: 3
  nodes: {}
  edges: []

agent:
  enable_history: true
  user_id: "default_user"
  graph_name: "template_agent"
  settings:
    max_tokens: 2048
    context_window: 16384
    temperature: 0.7
    top_p: 0.9
    frequency_penalty: 0.0
    presence_penalty: 0.0
  enable_logging: true
  prompt_section: "default"
  personality: null

tools:
  tool_timeout: 30
  max_retries: 3
  inherit_from_parent: true
  allowed_tools: []
  tool_descriptions: {}

personalities:
  default_personality: "default"
  personalities:
    default:
      name: "default"
      description: "Default personality"
      traits: []
      goals: []
      constraints: []
      system_prompt: ""
      examples: []
      enabled: false
      file_path: null
      use_by_default: true
